{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import utils\n",
    "import dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "# Below all hyper parameters assigned for the \n",
    "# convenience of notebook, however, it is not a best practice\n",
    "# as you should probably want to use hydra or argparse instead\n",
    "gpu_id=0\n",
    "seed=42\n",
    "save=0\n",
    "\n",
    "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "num_enc_layers=6\n",
    "num_dec_layers=6\n",
    "max_len=20\n",
    "model_dim=512\n",
    "hidden_size=2048\n",
    "d_k=64\n",
    "d_v=64\n",
    "n_head=8\n",
    "d_prob=0.1\n",
    "max_norm=5.0\n",
    "\n",
    "# attention is all you need hyper-params\n",
    "n_epochs=100\n",
    "batch_size=128\n",
    "lr=1e-3\n",
    "beta1=0.9\n",
    "beta2=0.98\n",
    "eps=1e-9\n",
    "weight_decay=1e-3\n",
    "teacher_forcing=False\n",
    "# bluescore\n",
    "k=4\n",
    "\n",
    "\n",
    "class Embedding_Layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_token, dim_model, max_seq_len, d_prob):\n",
    "        \"\"\"Implementation of embedding and positional embeddings of the Transformer.\n",
    "\n",
    "        Args:\n",
    "            num_token (int): Number of token\n",
    "            dim_model (_type_): Dimension of the model\n",
    "            max_seq_len (_type_): Maximum sequence length\n",
    "            d_prob (_type_): Dropout probability\n",
    "        \"\"\"\n",
    "        super(Embedding_Layer, self).__init__()\n",
    "        self.num_token = num_token\n",
    "        self.dim_model = dim_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_prob = d_prob\n",
    "        self.emb = nn.Embedding(num_token, dim_model)\n",
    "        self.drop_out = nn.Dropout(d_prob)\n",
    "        self.pos_enc = torch.zeros((self.max_seq_len, self.dim_model))\n",
    "        for pos in range(self.max_seq_len):\n",
    "            for idx in range(0, self.dim_model, 2):\n",
    "                self.pos_enc[pos, idx] = torch.sin(torch.tensor(pos / (10000.0) ** (float(idx) / self.dim_model)))\n",
    "                self.pos_enc[pos, idx + 1] = torch.cos(torch.tensor(pos / (10000.0) ** (float(idx) / self.dim_model)))\n",
    "\n",
    "    def forward(self, x, pos=None):\n",
    "        x = self.emb(x) * math.sqrt(self.dim_model)\n",
    "        if pos == None:\n",
    "            x += self.pos_enc.cuda()\n",
    "        else:\n",
    "            x += self.pos_enc[pos].cuda()\n",
    "        x = self.drop_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_prob):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.drop_out = nn.Dropout(d_prob)\n",
    "\n",
    "    def forward(self, x_q, x_k, x_v, pad_mask):\n",
    "        dot_product = torch.matmul(x_q, x_k.transpose(-1, -2))\n",
    "        scaled_dot_product = dot_product / math.sqrt(self.d_k)\n",
    "        if pad_mask != None:\n",
    "            scaled_dot_product = scaled_dot_product.masked_fill(pad_mask == False, -1e9)\n",
    "        reg_scaled_dot_product = self.softmax(scaled_dot_product)\n",
    "        reg_scaled_dot_product = self.drop_out(reg_scaled_dot_product)\n",
    "        scaled_dot_product_attn = torch.matmul(reg_scaled_dot_product, x_v)\n",
    "        return scaled_dot_product_attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_model, d_k, d_v, n_head, d_prob):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.dim_model = dim_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.w_q = nn.Linear(dim_model, n_head * d_k)\n",
    "        self.w_k = nn.Linear(dim_model, n_head * d_k)\n",
    "        self.w_v = nn.Linear(dim_model, n_head * d_v)\n",
    "        self.w_o = nn.Linear(n_head * d_v, dim_model)\n",
    "\n",
    "        self.scaled_dot_prod = ScaledDotProductAttention(d_k, d_prob)\n",
    "\n",
    "    def forward(self, q, k, v, pad_mask):\n",
    "        x_q = self.w_q(q).view(len(q), -1, self.n_head, self.d_k).transpose(1, 2)\n",
    "        x_k = self.w_k(k).view(len(k), -1, self.n_head, self.d_k).transpose(1, 2)\n",
    "        x_v = self.w_v(v).view(len(v), -1, self.n_head, self.d_v).transpose(1, 2)\n",
    "        if pad_mask != None:\n",
    "            pad_mask = pad_mask.unsqueeze(1)\n",
    "            pad_mask = pad_mask.expand(-1, self.n_head, -1, -1)\n",
    "        scaled_dot_product_attn = self.scaled_dot_prod(x_q, x_k, x_v, pad_mask)\n",
    "        scaled_dot_product_attn = scaled_dot_product_attn.transpose(1, 2)\n",
    "        scaled_dot_product_attn = scaled_dot_product_attn.reshape(len(v), -1, self.d_v * self.n_head)\n",
    "        output = self.w_o(scaled_dot_product_attn)\n",
    "        return output, q\n",
    "\n",
    "\n",
    "class MaskedScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_prob):\n",
    "        super(MaskedScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.drop_out = nn.Dropout(d_prob)\n",
    "\n",
    "    def forward(self, x_q, x_k, x_v, pad_mask):\n",
    "        dot_product = torch.matmul(x_q, x_k.transpose(-1, -2))\n",
    "        scaled_dot_product = dot_product / math.sqrt(self.d_k)\n",
    "        true_arr = torch.ones_like(scaled_dot_product)\n",
    "        if pad_mask != None:\n",
    "            mask = torch.tril(true_arr).bool()\n",
    "            scaled_dot_product = scaled_dot_product.masked_fill(mask==False, -1e9)\n",
    "            scaled_dot_product = scaled_dot_product.masked_fill(pad_mask==False, -1e9)\n",
    "        reg_scaled_dot_product = self.softmax(scaled_dot_product)\n",
    "        reg_scaled_dot_product = self.drop_out(reg_scaled_dot_product)\n",
    "        scaled_dot_product_attn = torch.matmul(reg_scaled_dot_product, x_v)\n",
    "        return scaled_dot_product_attn\n",
    "\n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_model, d_k, d_v, n_head, d_prob):\n",
    "        super(MaskedMultiHeadAttention, self).__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.w_q = nn.Linear(dim_model, n_head * d_k)\n",
    "        self.w_k = nn.Linear(dim_model, n_head * d_k)\n",
    "        self.w_v = nn.Linear(dim_model, n_head * d_v)\n",
    "        self.w_o = nn.Linear(n_head * d_v, dim_model)\n",
    "\n",
    "        self.masked_scaled_dot_prod = MaskedScaledDotProductAttention(d_k, d_prob)\n",
    "\n",
    "    def forward(self, q, k, v, pad_mask):\n",
    "        x_q = self.w_q(q).view(len(q), -1, self.n_head, self.d_k).transpose(1, 2)\n",
    "        x_k = self.w_k(k).view(len(k), -1, self.n_head, self.d_k).transpose(1, 2)\n",
    "        x_v = self.w_v(v).view(len(v), -1, self.n_head, self.d_v).transpose(1, 2)\n",
    "        if pad_mask != None:\n",
    "            pad_mask = pad_mask.unsqueeze(1)\n",
    "            pad_mask = pad_mask.expand(-1, self.n_head, -1, -1)\n",
    "        scaled_dot_product_attn = self.masked_scaled_dot_prod(x_q, x_k, x_v, pad_mask)\n",
    "        scaled_dot_product_attn = scaled_dot_product_attn.transpose(1, 2)\n",
    "        scaled_dot_product_attn = scaled_dot_product_attn.reshape(len(v), -1, self.d_v * self.n_head)\n",
    "        output = self.w_o(scaled_dot_product_attn)\n",
    "        return output, v\n",
    "\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, dim_model, dim_hidden, d_prob):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_model, dim_hidden)\n",
    "        self.fc2 = nn.Linear(dim_hidden, dim_model)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop_out = nn.Dropout(d_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc2(self.drop_out(self.relu(self.fc1(x))))\n",
    "        return output, x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, dim_model, d_k, d_v, n_head, dim_hidden, d_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_head = n_head\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.d_prob = d_prob\n",
    "\n",
    "        self.multi_head_attn = MultiHeadAttention(dim_model, d_k, d_v, n_head, d_prob)\n",
    "        self.ffnn = FFNN(dim_model, dim_hidden, d_prob)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(dim_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(dim_model)\n",
    "\n",
    "        self.drop_out = nn.Dropout(d_prob)\n",
    "\n",
    "    def forward(self, x, src_pad_mask):\n",
    "        x, x_residual = self.multi_head_attn(x, x, x, src_pad_mask) # Q = K = V\n",
    "        x = self.layer_norm1(x + x_residual)\n",
    "        x, x_residual = self.ffnn(x)\n",
    "        x = self.layer_norm2(x + x_residual)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_model, d_k, d_v, n_head, dim_hidden, d_prob, n_enc_layer):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_head = n_head\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.d_prob = d_prob\n",
    "        self.n_enc_layer = n_enc_layer\n",
    "\n",
    "        self.enc_layers = nn.ModuleList([EncoderLayer(dim_model, d_k, d_v, n_head, dim_hidden, d_prob) for _ in range(n_enc_layer)])\n",
    "\n",
    "    def forward(self, x, src_pad_mask):\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, src_pad_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dim_model, d_k, d_v, n_head, dim_hidden, d_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_head = n_head\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.d_prob = d_prob\n",
    "\n",
    "        self.masked_multi_head_attention = MaskedMultiHeadAttention(dim_model, d_k, d_v, n_head, d_prob)\n",
    "        self.multi_head_attention = MultiHeadAttention(dim_model, d_k, d_v, n_head, d_prob)\n",
    "        self.ffnn = FFNN(dim_model, dim_hidden, d_prob)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(dim_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(dim_model)\n",
    "        self.layer_norm3 = nn.LayerNorm(dim_model)\n",
    "\n",
    "        self.drop_out = nn.Dropout(d_prob)\n",
    "\n",
    "    def forward(self, x, enc_output, tgt_pad_mask, src_tgt_pad_mask):\n",
    "        x, x_residual = self.masked_multi_head_attention(x, x, x, tgt_pad_mask)  # Q = K = V\n",
    "        x = self.layer_norm1(x + x_residual)\n",
    "        x, x_residual = self.multi_head_attention(x, enc_output, enc_output, src_tgt_pad_mask)\n",
    "        x = self.layer_norm2(x + x_residual)\n",
    "        x, x_residual = self.ffnn(x)\n",
    "        x = self.layer_norm3(x + x_residual)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim_model, d_k, d_v, n_head, dim_hidden, d_prob, n_dec_layer):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_head = n_head\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.d_prob = d_prob\n",
    "        self.n_dec_layer = n_dec_layer\n",
    "\n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(dim_model, d_k, d_v, n_head, dim_hidden, d_prob) for _ in range(n_dec_layer)])\n",
    "\n",
    "    def forward(self, x, enc_output, tgt_pad_mask=None, src_tgt_pad_mask=None):\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x, enc_output, tgt_pad_mask, src_tgt_pad_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_token, max_seq_len, dim_model, d_k=64, d_v=64, n_head=8, dim_hidden=2048, d_prob=0.1, n_enc_layer=6, n_dec_layer=6):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.num_token = num_token\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embed = Embedding_Layer(num_token=num_token, dim_model=dim_model, max_seq_len=max_seq_len, d_prob=d_prob)\n",
    "        self.encoder = Encoder(dim_model, d_k, d_v, n_head, dim_hidden, d_prob, n_enc_layer)\n",
    "        self.decoder = Decoder(dim_model, d_k, d_v, n_head, dim_hidden, d_prob, n_dec_layer)\n",
    "        self.linear = nn.Linear(dim_model, num_token)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        positional_encoded_src = self.embed(src)\n",
    "        src_pad_mask = self.make_mask(src)\n",
    "        enc_output = self.encoder(positional_encoded_src, src_pad_mask.bool())\n",
    "\n",
    "        positional_encoded_tgt = self.embed(tgt)\n",
    "        tgt_pad_mask = self.make_mask(tgt)\n",
    "        tgt_src_pad_mask = torch.bmm(tgt_pad_mask, src_pad_mask.transpose(-1, -2))\n",
    "        dec_output = self.decoder(positional_encoded_tgt, enc_output, tgt_pad_mask.bool(), tgt_src_pad_mask.bool())\n",
    "        outputs = self.softmax(self.linear(dec_output))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def make_mask(self, x):\n",
    "        pad_mask = torch.where(x==2, 0., 1.)\n",
    "        pad_mask = pad_mask.unsqueeze(-1)\n",
    "        pad_mask = torch.bmm(pad_mask, pad_mask.transpose(-1, -2))\n",
    "        return pad_mask\n",
    "\n",
    "def train(dataloader, epochs, model, criterion, vocab, i2w):\n",
    "\tmodel.train()\n",
    "\tmodel.zero_grad()\n",
    "\toptimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, beta2), eps=eps)\n",
    "\tcorrect = 0\n",
    "\n",
    "\tcnt = 0\n",
    "\ttotal_score = 0.\n",
    "\tglobal_step = 0\n",
    "\ttr_loss = 0.\n",
    "\tfor epoch in range(epochs):\n",
    "\n",
    "\t\tfor idx, (src, tgt) in enumerate(dataloader):\n",
    "\t\t\tsrc, tgt = src.to(device), tgt.to(device)\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\toutputs = model(src, tgt[:, :-1])\n",
    "\t\t\ttgt = tgt[:, 1:]\n",
    "\n",
    "\t\t\ttgt = torch.flatten(tgt)\n",
    "\t\t\toutputs = outputs.reshape(len(tgt), -1)\n",
    "\t\t\tloss = criterion(outputs, tgt)\n",
    "\t\t\ttr_loss += loss.item()\n",
    "\n",
    "\t\t\tloss.backward()\n",
    "\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tglobal_step += 1\n",
    "\n",
    "\t\t\tpred = outputs.argmax(dim=1, keepdim=True)\n",
    "\t\t\tpred_acc = pred[tgt != 2]\n",
    "\t\t\ttgt_acc = tgt[tgt != 2]\n",
    "\n",
    "\t\t\tcorrect += pred_acc.eq(tgt_acc.view_as(pred_acc)).sum().item()\n",
    "\n",
    "\t\t\tcnt += tgt_acc.shape[0]\n",
    "\t\t\tscore = 0.\n",
    "\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tpred = pred.reshape(batch_size, max_len, -1).detach().cpu().tolist()\n",
    "\t\t\t\ttgt = tgt.reshape(batch_size, max_len).detach().cpu().tolist()\n",
    "\t\t\t\tfor p, t in zip(pred, tgt):\n",
    "\t\t\t\t\teos_idx = t.index(vocab['[PAD]']) if vocab['[PAD]'] in t else len(t)\n",
    "\t\t\t\t\tp_seq = [i2w[i[0]] for i in p][:eos_idx]\n",
    "\t\t\t\t\tt_seq = [i2w[i] for i in t][:eos_idx]\n",
    "\t\t\t\t\tk = 4 if len(t_seq) > 4 else len(t_seq)\n",
    "\t\t\t\t\ts = utils.bleu_score(p_seq, t_seq, k=k)\n",
    "\t\t\t\t\tscore += s\n",
    "\t\t\t\t\ttotal_score += s\n",
    "\n",
    "\t\t\tscore /= batch_size\n",
    "\n",
    "\t\t\tprint(\"\\r[epoch {:3d}/{:3d}] [batch {:4d}/{:4d}] loss: {:.6f} acc: {:.4f} BLEU: {:.4f})\".format(\n",
    "\t\t\t\tepoch, n_epochs, idx + 1, len(dataloader), loss, correct / cnt, score), end=' ')\n",
    "\n",
    "\ttr_loss /= cnt\n",
    "\ttr_acc = correct / cnt\n",
    "\ttr_score = total_score / len(dataloader.dataset) / epochs\n",
    "\n",
    "\treturn tr_loss, tr_acc, tr_score\n",
    "\n",
    "def eval(dataloader, model, lengths=None):\n",
    "    # Implement this\n",
    "    pass      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.set_random_seed(42)\n",
    "trn_dataset = dataloader.MiniWMT15_en_ru_Dataset(max_len=20, src_filepath='../data/src_train.txt', tgt_filepath='../data/tgt_train.txt', vocab=(None, None), is_src=True, is_tgt=False, is_train=True)\n",
    "test_dataset = dataloader.MiniWMT15_en_ru_Dataset(max_len=20, src_filepath='../data/src_test.txt', tgt_filepath=None, vocab=(trn_dataset.vocab, None), is_src=True, is_tgt=False, is_train=False)\n",
    "vocab = trn_dataset.vocab\n",
    "i2w = {v: k for k, v in vocab.items()}\n",
    "trn_dataloader = DataLoader(trn_dataset, batch_size=128, shuffle=True, drop_last=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
    "n_token = len(trn_dataset.vocab)\n",
    "model = Transformer(num_token=n_token, max_seq_len=max_len, dim_model=model_dim, d_k=d_k, d_v=d_v, n_head=n_head, dim_hidden=hidden_size, d_prob=d_prob, n_enc_layer=num_enc_layers, n_dec_layer=num_dec_layers)\n",
    "model = model.to(device)\n",
    "criterion = nn.NLLLoss(ignore_index=vocab['[PAD]'])\n",
    "\n",
    "tr_loss, tr_acc, tr_score = train(trn_dataloader, n_epochs, model, criterion, vocab, i2w)\n",
    "print(\"tr: ({:.4f}, {:5.2f}, {:5.2f}) | \".format(tr_loss, tr_acc * 100, tr_score * 100), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dlub_2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0020dc2bff844068f9b5b1ead3ea97b8c4f02721b75096722229b3552c22662"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
